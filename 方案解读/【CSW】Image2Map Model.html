<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑 Light" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/606060 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑 Light;
      font-size: 14pt;
    }
  </style>
</head>
<body>
<a name="16870"/>
<h1>【CSW】Image2Map Model</h1>
<div>
<table bgcolor="#D4DDE5" border="0">
<tr><td><b>创建时间：</b></td><td><i>2021/11/12 13:10</i></td></tr>
<tr><td><b>更新时间：</b></td><td><i>2022/12/13 18:00</i></td></tr>
<tr><td><b>作者：</b></td><td><i>不理不理</i></td></tr>
</table>
</div>
<br/>

<div>
<span><div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">模型结构</span></div><div>        冠军方案中的Image2Map网络大致可以分为三部分：Encoder、Decoder、Coord_map。</div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/map_model_part.png" type="image/png" data-filename="map_model_part.png" width="417"/></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/Untitled Diagram-Page-1.drawio.png" type="image/png" data-filename="Untitled Diagram-Page-1.drawio.png"/></div><div style="text-align: center;"><br/></div><div style="text-align: left;">        关于Global Map和两个缩放比的理解：</div><ul><ul><li><div style="text-align: left;">首先为什么需要用Global Map X和Global Map Y来表示原图中某个像素点的xy坐标？是因为仅用一个Global Map无法表示x和y坐标，因为Global Map矩阵中任意一个像素点处的值仅有一个，而不是两个。</div></li><li><div style="text-align: left;">缩放比和长宽比是如何起作用的？如图所示，Global Map X和Global Map Y分别按从左到右的0到1矩阵和从下到上的0到1矩阵初始化，因此原图左下角点的预测坐标为（0,0），右下角点的预测坐标为（1,0），左上角点的预测坐标为（0,1），右上角点的预测坐标为（1,1）；当Global Map矩阵整体乘以缩放比时，原图中每个像素的预测坐标都会等比例缩放，若缩放比是3，那么左下角点的预测坐标为（0,0），右下角点的预测坐标为（3,0），左上角点的预测坐标为（0,3），右上角点的预测坐标为（3,3）；当Global Map Y矩阵再整体乘以长宽比时，原图中每个像素的预测坐标y都会等比例缩放，若长宽比是2，那么左下角点的预测坐标为（0,0），右下角点的预测坐标为（3,0），左上角点的预测坐标为（0,6），右上角点的预测坐标为（3,6），整个矩形就被拉高了。</div></li><li><div style="text-align: left;"><br/></div></li></ul></ul><div style="text-align: left;"><br/></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/Untitled Diagram-Page-1.drawio [1].png" type="image/png" data-filename="Untitled Diagram-Page-1.drawio.png"/></div><div style="text-align: left;"><br/></div><div style="text-align: left;">        Encoder部分通过Effv2s模型对输入图片进行编码，提取出各种层级的特征。</div><div style="text-align: left;"><br/></div><div style="text-align: left;">        Decoder部分额外引入该图片bbox的平均尺寸，连同Encoder网络提取的图片特征，来推断坐标系的缩放比和长宽比。同时不断地将高层级的特征图上采样后与低层级的特征图融合，形成兼具位置信息与语义信息的RGB特征图。</div><div style="text-align: left;"><br/></div><div style="text-align: left;">        Coord_map也可以分为提取坐标与提取特征两个部分。使用Decoder部分提取的RGB特征图，来预测图片中每个像素点相对于全局的坐标偏移量，结合按一定规则构建的全局坐标，可以得出图片中每个像素点的全局坐标。然后按一定规则扩展图片中的每个bbox，使得扩展后的bbox可以涵盖运动员的身形，从RGB特征图中抠取每个运动员所在位置（扩展后的bbox）的特征图，结合该运动员自身的头盔位置掩码及其他周围运动员的头盔位置掩码，精炼出一个权重矩阵，权重矩阵中的值表明了全局坐标矩阵中每个像素对应坐标的重要性。该权重矩阵将会与该运动员的全局坐标矩阵进行加权平均，求出运动员最终的坐标位置。</div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/下载.png" type="image/png" data-filename="下载.png"/></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">模型训练</span></div><div>        Q：Why did you multiply xy_offsets matrix with epoch_dependent_value i don't see any relation between the number of training epochs and xy_offsets?</div><div>              xy_offsets = Lambda(lambda x: x*epoch_dependent_value, name=&quot;epoch_dependent&quot;)(xy_offsets).</div><div>        A：Delta map(xy_offsets) starts training from approx. 5 epoch. Only global map is trained in first 5 epochs. So epoch_dependent_value is 0.0 if epoch&lt;5, otherwise 1.0. Actually, this model works without the global map, since the delta map can include it. But I believed using the former leads to the better prediction. So I forced the model to learn the location without the delta map for the first several epochs.</div><div><br/></div><div>        所以训练的前5回合，是没有delta map参与的，只有global <span style="font-size: unset; color: unset; font-family: unset;">map。</span></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">函数add_coords（节点add_coords）</span></div><div>        channel方面新增的2个16x28的<span style="font-size: unset; color: unset; font-family: unset;">特征图分别为：</span></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/Image.png" type="image/png" data-filename="Image.png" width="852"/></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/h_grid.png" type="image/png" data-filename="h_grid.png" style="color: unset; font-family: unset; font-size: unset;" width="565"/></div><div style="text-align: center;"><br/></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/Image [1].png" type="image/png" data-filename="Image.png"/></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/w_grid.png" type="image/png" data-filename="w_grid.png" style="color: unset; font-family: unset; font-size: unset;"/></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">函数add_scale_aspect（节点add_scale_aspect）</span></div><div>        channel方面新增的2个特征图分别为：<span style="font-size: unset; color: unset; font-family: unset;">值都为scale_ratio的（16, 28）矩阵，以及值都为aspect_ratio的（16, 28）矩阵。</span></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">函数xy_conbine_layer（节点xy_combine）</span></div><div>        传入的数据中会提供2个形状为（128, 224）的特征图xy_offsets[:, :, :, 0]和xy_offsets[:, :, :, 1]，两个特征图分别对应X轴坐标和Y轴坐标的偏移量，特征图中每个值的值域为[0，1]。</div><div><br/></div><div>        其次，该函数还会生成两个形状为（128, 224）的特征图grid，分别对应X轴的绝对坐标和Y轴的绝对坐标</div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/Image [2].png" type="image/png" data-filename="Image.png" width="644"/></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/grid_x.png" type="image/png" data-filename="grid_x.png"/></div><div style="text-align: center;"><br/></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/Image [3].png" type="image/png" data-filename="Image.png" width="649"/></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/grid_y.png" type="image/png" data-filename="grid_y.png"/></div><div><br/></div><div>        该函数返回的数据xy，xy[:, :, :, 0] = (grid_x + xy_offsets[:, :, :, 0]) * scale_ratio</div><div>                                          xy[:, :, :, 1] = (grid_y + xy_offsets[:, :, :, 1]) * scale_ratio * aspect_ratio</div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">函数larger_crop_resize_layer（节点crop_bbox和crop_bbox_attention</span><span style="font-size: 18pt; color: unset; font-family: unset; font-weight: bold;">）</span></div><div>        该函数接受三个输入，尺寸分别为（h, w, c）的特征图images、（num_box, 4）的归一化后检测框坐标bbox、（1, ）的归一化后的检测框平均大小met_size，这个函数会提取放大版检测框（考虑到头盔与人的位置关系，故作下面的变换：top=center_y-1.5*met_size, bottom=center_y+3*met_size, left=center_x-1.5*met_size, right=center_y+1.5*met_size）在images中对应位置处的特征图，并将其缩放到（24, 16, c），最后函数返回一个（num_box, 24, 16, c）的张量。</div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/20190117161013486.jpg" type="image/jpeg" data-filename="20190117161013486.jpg" width="436"/><img src="【CSW】Image2Map Model_files/20190117161135488.png" type="image/png" data-filename="20190117161135488.png" width="327"/></div><div><br/></div><div>        在map模型中，第一次使用时该函数完成了上述功能；第二次使用时，因为<span style="font-size: medium; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(0, 0, 0); font-family: &quot;微软雅黑 Light&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">add_crop_mask=True，所以</span><span style="font-size: unset; color: unset; font-family: unset;">将头盔坐标，从图片左上角点坐标系，转化到以放大版检测框左上点坐标系下，然后在（24, 16）的特征图中，将头盔所在位置标为1，其余位置标为0，形成了头盔的位置掩码。</span></div><div><br/></div><div><span style="color: unset; font-family: unset;"><span style="font-size: unset; color: unset; font-family: unset;">    </span>    返回的（num_bbox, 24, 16, 18）数组中，前16个信道是从原始的（128, 224, 16）rgb特征图中抠取放大版检测框并resize后得到运动员的rgb特征图。第17个信<span style="font-size: 14pt; color: unset; font-family: unset;">道是从【</span></span><span style="font-size: 14pt;">函数add_bbox_img</span><span style="font-size: 14pt; color: unset; font-family: unset;">】添加的bbox位置掩码抠取并resize的结果，对应第三个图。第18个信道是只有该运动员对应bbox的位置掩码，对应第四个图。（前两个图为了清晰，并没有resize至（24, 16），而是</span><span style="font-size: 14pt; color: unset; font-family: unset;">（96, 64）</span><span style="font-size: 14pt; color: unset; font-family: unset;">）</span></div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/下载 [1].png" type="image/png" data-filename="下载.png"/></div><div><br/></div><div>        其中，图三对应的矩阵如下图所示，由于crop后需要resize至（24, 16）的缘故，线性插值产生了一些浮点数。</div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/Image [4].png" type="image/png" data-filename="Image.png" width="1085"/></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">函数add_bbox_img（节点add_box_mask）</span></div><div>        将该输入特征图对应的所有bbox，依据bbox的归一化坐标，映射到（128, 224）的下图中去，没有bbox的地方为0，只有单个bbox的地方则为1，bbox重叠的地方值累加，然后将新生成的特征图拼接到原有特征图后面。</div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/image_all_mask.png" type="image/png" data-filename="image_all_mask.png" width="710"/></div><div style="text-align: center;"><br/></div><div style="text-align: left;">        在原图中表现为如下形式：</div><div style="text-align: center;"><img src="【CSW】Image2Map Model_files/下载 [2].png" type="image/png" data-filename="下载.png"/></div><div><br/></div></div><div><br/></div></span>
</div></body></html> 